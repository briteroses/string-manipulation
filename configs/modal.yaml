modal:
  model_dir: "/huggingface_models"
  gpu_config_kwargs:
    size: "40GB"
    count: 1
  model_preloads: []
    # - "meta-llama/Llama-2-7b-hf"
    # - "meta-llama/Llama-2-7b-chat-hf"
    # - "meta-llama/Llama-2-13b-hf"
    # - "meta-llama/Llama-2-13b-chat-hf"
    # - "meta-llama/Llama-2-70b-hf"
    # - "meta-llama/Llama-2-70b-chat-hf"
    # - "meta-llama/Meta-Llama-3-8B"
    # - "meta-llama/Meta-Llama-3-8B-Instruct"
    # - "meta-llama/Meta-Llama-3-70B"
    # - "meta-llama/Meta-Llama-3-70B-Instruct"
    # - "google/gemma-2b"
    # - "google/gemma-2b-it"
    # - "google/gemma-7b"
    # - "google/gemma-7b-it"
    # - "mistralai/Mistral-7B-v0.1"
    # - "mistralai/Mistral-7B-Instruct-v0.2"
    # - "mistralai/Mixtral-8x7B-v0.1"
    # - "mistralai/Mixtral-8x7B-Instruct-v0.1"
    # - "mistralai/Mixtral-8x22B-v0.1"
    # - "mistralai/Mixtral-8x22B-Instruct-v0.1"
    # - "CohereForAI/c4ai-command-r-plus"
    # - "CohereForAI/c4ai-command-r-v01"
    # - "databricks/dbrx-base"
    # - "databricks/dbrx-instruct"
    # - "Qwen/Qwen1.5-0.5B"
    # - "Qwen/Qwen1.5-0.5B-Chat"
    # - "Qwen/Qwen1.5-1.8B"
    # - "Qwen/Qwen1.5-1.8B-Chat"
    # - "Qwen/Qwen1.5-4B"
    # - "Qwen/Qwen1.5-4B-Chat"
    # - "Qwen/Qwen1.5-7B"
    # - "Qwen/Qwen1.5-7B-Chat"
    # - "Qwen/Qwen1.5-14B"
    # - "Qwen/Qwen1.5-14B-Chat"
    # - "Qwen/Qwen1.5-32B"
    # - "Qwen/Qwen1.5-32B-Chat"
    # - "Qwen/Qwen1.5-72B"
    # - "Qwen/Qwen1.5-72B-Chat"
    # - "Qwen/Qwen1.5-110B"
    # - "Qwen/Qwen1.5-110B-Chat"
    # - "Qwen/Qwen1.5-MoE-A2.7B"
    # - "Qwen/Qwen1.5-MoE-A2.7B-Chat"
