# NOTE: attack success rate correlates to the max tokens allowed in completion
# (this was found in HarmBench from Dan Hendrycks' team)
# so for fairness in experiments, try to use the same max tokens across all models
max_tokens: &MAXTOK 256

openai:
  inference_api:
    temperature: 1
    max_tokens: *MAXTOK
    n: 1
  legacy_inference_api:
    temperature: 1
    max_tokens: *MAXTOK
    n: 1
  system_prompt: "You are a helpful AI assistant."
  max_api_retries: 5
gemini:
  inference_api:
    candidate_count: 1
    max_output_tokens: *MAXTOK
    temperature: 0.9
    top_p: 1
    top_k: 50000 # what's the real vocab size?
cohere:
  inference_api:
    temperature: 1
    max_tokens: *MAXTOK
    n: 1
  system_prompt: "You are a helpful AI assistant."


opensource:
  bitsandbytes:
    load_in_8bit: True
  inference:
    max_new_tokens: *MAXTOK
    temperature: 1.0
    top_k: 50
    top_p: 1.0
    output_scores: False
    return_dict_in_generate: True
  mpt_only:
    system_prompt: "You are a helpful AI assistant."